{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88c2bbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"cifar10\"\n",
    "algorithm = \"crown ibp\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0665ea7",
   "metadata": {},
   "source": [
    "### 1. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32e937f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from art.utils import load_cifar10\n",
    "import numpy as np\n",
    "\n",
    "(_x1, _y1), (_x2, _y2), _min, _max = load_cifar10()\n",
    "x = np.concatenate((_x1, _x2))\n",
    "y = np.concatenate((_y1, _y2))\n",
    "\n",
    "# Swap axes to PyTorch's NCHW format\n",
    "x = np.transpose(x, (0, 3, 1, 2)).astype(np.float32)\n",
    "\n",
    "target_train_size = 2500\n",
    "target_test_size = 2500\n",
    "x_target_train = x[:target_train_size]\n",
    "y_target_train = y[:target_train_size]\n",
    "x_target_test = x[target_train_size:target_train_size+target_test_size]\n",
    "y_target_test = y[target_train_size:target_train_size+target_test_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfa1313",
   "metadata": {},
   "source": [
    "### 2. Define Target Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df7910fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 16, 16]             784\n",
      "              ReLU-2           [-1, 16, 16, 16]               0\n",
      "            Conv2d-3             [-1, 32, 8, 8]           8,224\n",
      "              ReLU-4             [-1, 32, 8, 8]               0\n",
      "           Flatten-5                 [-1, 2048]               0\n",
      "            Linear-6                  [-1, 100]         204,900\n",
      "              ReLU-7                  [-1, 100]               0\n",
      "            Linear-8                   [-1, 10]           1,010\n",
      "================================================================\n",
      "Total params: 214,918\n",
      "Trainable params: 214,918\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.11\n",
      "Params size (MB): 0.82\n",
      "Estimated Total Size (MB): 0.94\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from torchsummary import summary\n",
    "import math\n",
    "\n",
    "def cifar_model(): \n",
    "    model = nn.Sequential(\n",
    "        nn.Conv2d(3, 16, 4, stride=2, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(16, 32, 4, stride=2, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(32*8*8,100),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(100, 10)\n",
    "    )\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "            m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            m.bias.data.zero_()\n",
    "    return model\n",
    "\n",
    "summary(cifar_model(), input_size=x_target_train[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66db6326",
   "metadata": {},
   "source": [
    "### 3. Train Target Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8126fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO     17:15:15     Namespace(batch_size=64, bound_opts=None, bound_type='CROWN-IBP', clip_grad_norm=8.0, data='CIFAR', device='cpu', eps=0.03137254901960784, load='', lr=0.0005, lr_decay_milestones=[70, 85], lr_decay_rate=0.1, model='cifar_model', no_loss_fusion=False, norm=2, num_epochs=100, scheduler_name='SmoothedScheduler', scheduler_opts='start=10,length=61,mid=0.4', seed=100, verify=False)\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "\u001b[91m[WARN] Cannot find rule for <class 'models.feedforward.Flatten'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "\u001b[91m[WARN] Cannot find rule for <class 'torch.nn.modules.container.Sequential'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
      "INFO     17:15:27     macs: 1865680.0, params: 214918.0\n",
      "INFO     17:15:27     Sequential(\n",
      "  (0): Conv2d(3, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (2): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (3): ReLU()\n",
      "  (4): Flatten()\n",
      "  (5): Linear(in_features=2048, out_features=100, bias=True)\n",
      "  (6): ReLU()\n",
      "  (7): Linear(in_features=100, out_features=10, bias=True)\n",
      ")\n",
      "INFO     17:15:27     Epoch 1, learning rate [0.0005]\n",
      "INFO     17:15:32     [ 1:  40]: eps=0.000000000000 CE=2.0394 grad_norm=0.8601 Loss=2.0394 Time=0.0117\n",
      "INFO     17:15:32     Epoch time: 5.3987, Total time: 5.3987\n",
      "INFO     17:15:32     Evaluating...\n",
      "INFO     17:15:36     [ 1:  40]: eps=0.000000000000 CE=2.1295 Err=0.7552 Loss=2.1295 Time=0.0040\n",
      "INFO     17:15:36     Epoch 2, learning rate [0.0005]\n",
      "INFO     17:15:41     [ 2:  40]: eps=0.000000000000 CE=1.6150 grad_norm=3.3057 Loss=1.6150 Time=0.0086\n",
      "INFO     17:15:41     Epoch time: 4.3459, Total time: 9.7446\n",
      "INFO     17:15:41     Evaluating...\n",
      "INFO     17:15:45     [ 2:  40]: eps=0.000000000000 CE=1.9630 Err=0.7068 Loss=1.9630 Time=0.0049\n",
      "INFO     17:15:45     Epoch 3, learning rate [0.0005]\n",
      "INFO     17:15:50     [ 3:  40]: eps=0.000000000000 CE=1.3816 grad_norm=4.6764 Loss=1.3816 Time=0.0110\n",
      "INFO     17:15:50     Epoch time: 5.1872, Total time: 14.9318\n",
      "INFO     17:15:50     Evaluating...\n",
      "INFO     17:15:55     [ 3:  40]: eps=0.000000000000 CE=1.8865 Err=0.6816 Loss=1.8865 Time=0.0049\n",
      "INFO     17:15:55     Epoch 4, learning rate [0.0005]\n",
      "INFO     17:16:00     [ 4:  40]: eps=0.000000000000 CE=1.3092 grad_norm=4.4678 Loss=1.3092 Time=0.0109\n",
      "INFO     17:16:00     Epoch time: 4.9080, Total time: 19.8398\n",
      "INFO     17:16:00     Evaluating...\n",
      "INFO     17:16:04     [ 4:  40]: eps=0.000000000000 CE=1.7924 Err=0.6340 Loss=1.7924 Time=0.0051\n",
      "INFO     17:16:04     Epoch 5, learning rate [0.0005]\n",
      "INFO     17:16:09     [ 5:  40]: eps=0.000000000000 CE=1.1469 grad_norm=4.8429 Loss=1.1469 Time=0.0106\n",
      "INFO     17:16:09     Epoch time: 4.8229, Total time: 24.6628\n",
      "INFO     17:16:09     Evaluating...\n",
      "INFO     17:16:14     [ 5:  40]: eps=0.000000000000 CE=1.7702 Err=0.6324 Loss=1.7702 Time=0.0066\n",
      "INFO     17:16:14     Epoch 6, learning rate [0.0005]\n",
      "INFO     17:16:19     [ 6:  40]: eps=0.000000000000 CE=1.1190 grad_norm=6.5799 Loss=1.1190 Time=0.0120\n",
      "INFO     17:16:19     Epoch time: 5.4991, Total time: 30.1618\n",
      "INFO     17:16:19     Evaluating...\n",
      "INFO     17:16:24     [ 6:  40]: eps=0.000000000000 CE=1.7536 Err=0.6240 Loss=1.7536 Time=0.0050\n",
      "INFO     17:16:24     Epoch 7, learning rate [0.0005]\n",
      "INFO     17:16:30     [ 7:  40]: eps=0.000000000000 CE=1.0732 grad_norm=5.0098 Loss=1.0732 Time=0.0144\n",
      "INFO     17:16:30     Epoch time: 6.1598, Total time: 36.3217\n",
      "INFO     17:16:30     Evaluating...\n",
      "INFO     17:16:36     [ 7:  40]: eps=0.000000000000 CE=1.6979 Err=0.6044 Loss=1.6979 Time=0.0091\n",
      "INFO     17:16:36     Epoch 8, learning rate [0.0005]\n",
      "INFO     17:16:42     [ 8:  40]: eps=0.000000000000 CE=1.0258 grad_norm=5.6465 Loss=1.0258 Time=0.0157\n",
      "INFO     17:16:42     Epoch time: 5.8438, Total time: 42.1655\n",
      "INFO     17:16:42     Evaluating...\n",
      "INFO     17:16:47     [ 8:  40]: eps=0.000000000000 CE=1.7144 Err=0.6236 Loss=1.7144 Time=0.0074\n",
      "INFO     17:16:47     Epoch 9, learning rate [0.0005]\n",
      "INFO     17:16:52     [ 9:  40]: eps=0.000000000000 CE=0.9850 grad_norm=5.9859 Loss=0.9850 Time=0.0128\n",
      "INFO     17:16:52     Epoch time: 5.0292, Total time: 47.1947\n",
      "INFO     17:16:52     Evaluating...\n",
      "INFO     17:16:57     [ 9:  40]: eps=0.000000000000 CE=1.6630 Err=0.5820 Loss=1.6630 Time=0.0053\n",
      "INFO     17:16:57     Epoch 10, learning rate [0.0005]\n",
      "INFO     17:17:04     [10:  40]: eps=0.000000011426 CE=0.0258 grad_norm=5.5831 Loss=1.5024 Time=0.0725 Robust_CE=1.5154\n",
      "INFO     17:17:04     Epoch time: 7.3694, Total time: 54.5641\n",
      "INFO     17:17:04     Evaluating...\n",
      "INFO     17:17:10     [10:  40]: eps=0.000000011426 CE=1.6785 Err=0.5880 Loss=1.6785 Robust_CE=1.6785 Verified_Err=0.5880 Time=0.0165\n",
      "INFO     17:17:10     Epoch 11, learning rate [0.0005]\n",
      "INFO     17:17:17     [11:  40]: eps=0.000000192380 CE=0.0000 grad_norm=5.7801 Loss=1.4738 Robust_CE=1.4738 Time=0.0734\n",
      "INFO     17:17:17     Epoch time: 7.5151, Total time: 62.0792\n",
      "INFO     17:17:17     Evaluating...\n",
      "INFO     17:17:23     [11:  40]: eps=0.000000192380 CE=1.6419 Err=0.5772 Loss=1.6420 Robust_CE=1.6420 Verified_Err=0.5772 Time=0.0218\n",
      "INFO     17:17:23     Epoch 12, learning rate [0.0005]\n",
      "INFO     17:17:30     [12:  40]: eps=0.000000990468 CE=0.0000 grad_norm=6.6600 Loss=1.4596 Robust_CE=1.4596 Time=0.0738\n",
      "INFO     17:17:30     Epoch time: 7.3851, Total time: 69.4643\n",
      "INFO     17:17:30     Evaluating...\n",
      "INFO     17:17:35     [12:  40]: eps=0.000000990468 CE=1.6616 Err=0.5940 Loss=1.6622 Robust_CE=1.6622 Verified_Err=0.5944 Time=0.0165\n",
      "INFO     17:17:35     Epoch 13, learning rate [0.0005]\n",
      "INFO     17:17:43     [13:  40]: eps=0.000003156756 CE=0.0000 grad_norm=6.0360 Loss=1.4349 Robust_CE=1.4349 Time=0.0753\n",
      "INFO     17:17:43     Epoch time: 7.6431, Total time: 77.1074\n",
      "INFO     17:17:43     Evaluating...\n",
      "INFO     17:17:48     [13:  40]: eps=0.000003156756 CE=1.6406 Err=0.5948 Loss=1.6425 Robust_CE=1.6425 Verified_Err=0.5952 Time=0.0186\n",
      "INFO     17:17:48     Epoch 14, learning rate [0.0005]\n",
      "INFO     17:17:55     [14:  40]: eps=0.000007745774 CE=0.0000 grad_norm=7.1527 Loss=1.4019 Robust_CE=1.4019 Time=0.0724\n",
      "INFO     17:17:55     Epoch time: 7.4607, Total time: 84.5682\n",
      "INFO     17:17:55     Evaluating...\n",
      "INFO     17:18:01     [14:  40]: eps=0.000007745774 CE=1.7512 Err=0.6060 Loss=1.7564 Robust_CE=1.7564 Verified_Err=0.6084 Time=0.0171\n",
      "INFO     17:18:01     Epoch 15, learning rate [0.0005]\n",
      "INFO     17:18:08     [15:  40]: eps=0.000016115512 CE=0.0000 grad_norm=7.9405 Loss=1.4086 Robust_CE=1.4086 Time=0.0751\n",
      "INFO     17:18:08     Epoch time: 7.7302, Total time: 92.2984\n",
      "INFO     17:18:08     Evaluating...\n",
      "INFO     17:18:14     [15:  40]: eps=0.000016115512 CE=1.7019 Err=0.6084 Loss=1.7126 Robust_CE=1.7126 Verified_Err=0.6144 Time=0.0171\n",
      "INFO     17:18:14     Epoch 16, learning rate [0.0005]\n",
      "INFO     17:18:21     [16:  40]: eps=0.000029927422 CE=0.0000 grad_norm=7.5259 Loss=1.3724 Robust_CE=1.3724 Time=0.0760\n",
      "INFO     17:18:21     Epoch time: 7.5571, Total time: 99.8554\n",
      "INFO     17:18:21     Evaluating...\n",
      "INFO     17:18:27     [16:  40]: eps=0.000029927422 CE=1.7084 Err=0.6116 Loss=1.7269 Robust_CE=1.7269 Verified_Err=0.6192 Time=0.0168\n",
      "INFO     17:18:27     Epoch 17, learning rate [0.0005]\n",
      "INFO     17:18:35     [17:  40]: eps=0.000051146419 CE=0.0000 grad_norm=6.7236 Loss=1.3560 Robust_CE=1.3560 Time=0.0711\n",
      "INFO     17:18:35     Epoch time: 7.5201, Total time: 107.3755\n",
      "INFO     17:18:35     Evaluating...\n",
      "INFO     17:18:40     [17:  40]: eps=0.000051146419 CE=1.6471 Err=0.5980 Loss=1.6818 Robust_CE=1.6818 Verified_Err=0.6120 Time=0.0184\n",
      "INFO     17:18:40     Epoch 18, learning rate [0.0005]\n",
      "INFO     17:18:47     [18:  40]: eps=0.000082040876 CE=0.0000 grad_norm=5.2215 Loss=1.3101 Robust_CE=1.3101 Time=0.0672\n",
      "INFO     17:18:47     Epoch time: 7.2155, Total time: 114.5910\n",
      "INFO     17:18:47     Evaluating...\n",
      "INFO     17:18:53     [18:  40]: eps=0.000082040876 CE=1.6001 Err=0.5616 Loss=1.6574 Robust_CE=1.6574 Verified_Err=0.5888 Time=0.0167\n",
      "INFO     17:18:53     Epoch 19, learning rate [0.0005]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO     17:19:01     [19:  40]: eps=0.000125182632 CE=0.0000 grad_norm=5.1099 Loss=1.2807 Robust_CE=1.2807 Time=0.0853\n",
      "INFO     17:19:01     Epoch time: 8.0152, Total time: 122.6062\n",
      "INFO     17:19:01     Evaluating...\n",
      "INFO     17:19:07     [19:  40]: eps=0.000125182632 CE=1.6096 Err=0.5676 Loss=1.6987 Robust_CE=1.6987 Verified_Err=0.6072 Time=0.0207\n",
      "INFO     17:19:07     Epoch 20, learning rate [0.0005]\n",
      "INFO     17:19:15     [20:  40]: eps=0.000183446984 CE=0.0000 grad_norm=5.9040 Loss=1.2764 Robust_CE=1.2764 Time=0.0942\n",
      "INFO     17:19:15     Epoch time: 8.9074, Total time: 131.5136\n",
      "INFO     17:19:15     Evaluating...\n",
      "INFO     17:19:21     [20:  40]: eps=0.000183446984 CE=1.6343 Err=0.5720 Loss=1.7723 Robust_CE=1.7723 Verified_Err=0.6264 Time=0.0184\n",
      "INFO     17:19:21     Epoch 21, learning rate [0.0005]\n",
      "INFO     17:19:29     [21:  40]: eps=0.000260012693 CE=0.0000 grad_norm=6.5235 Loss=1.2558 Robust_CE=1.2558 Time=0.0764\n",
      "INFO     17:19:29     Epoch time: 7.6271, Total time: 139.1407\n",
      "INFO     17:19:29     Evaluating...\n",
      "INFO     17:19:34     [21:  40]: eps=0.000260012693 CE=1.6081 Err=0.5748 Loss=1.7963 Robust_CE=1.7963 Verified_Err=0.6456 Time=0.0200\n",
      "INFO     17:19:34     Epoch 22, learning rate [0.0005]\n",
      "INFO     17:19:42     [22:  40]: eps=0.000358361980 CE=0.0000 grad_norm=6.5044 Loss=1.2604 Robust_CE=1.2604 Time=0.0768\n",
      "INFO     17:19:42     Epoch time: 7.6647, Total time: 146.8054\n",
      "INFO     17:19:42     Evaluating...\n",
      "INFO     17:19:47     [22:  40]: eps=0.000358361980 CE=1.5948 Err=0.5712 Loss=1.8545 Robust_CE=1.8545 Verified_Err=0.6616 Time=0.0179\n",
      "INFO     17:19:47     Epoch 23, learning rate [0.0005]\n",
      "INFO     17:19:55     [23:  40]: eps=0.000482280528 CE=0.0000 grad_norm=5.4338 Loss=1.2559 Robust_CE=1.2559 Time=0.0778\n",
      "INFO     17:19:55     Epoch time: 7.5458, Total time: 154.3512\n",
      "INFO     17:19:55     Evaluating...\n",
      "INFO     17:20:00     [23:  40]: eps=0.000482280528 CE=1.5942 Err=0.5664 Loss=1.9499 Robust_CE=1.9499 Verified_Err=0.6896 Time=0.0189\n",
      "INFO     17:20:00     Epoch 24, learning rate [0.0005]\n",
      "INFO     17:20:08     [24:  40]: eps=0.000635857481 CE=0.0000 grad_norm=5.6908 Loss=1.2755 Robust_CE=1.2755 Time=0.0752\n",
      "INFO     17:20:08     Epoch time: 7.6309, Total time: 161.9822\n",
      "INFO     17:20:08     Evaluating...\n",
      "INFO     17:20:13     [24:  40]: eps=0.000635857481 CE=1.5954 Err=0.5692 Loss=2.0254 Robust_CE=2.0254 Verified_Err=0.7000 Time=0.0187\n",
      "INFO     17:20:13     Epoch 25, learning rate [0.0005]\n",
      "INFO     17:20:21     [25:  40]: eps=0.000823485447 CE=0.0000 grad_norm=5.9666 Loss=1.3185 Robust_CE=1.3185 Time=0.0770\n",
      "INFO     17:20:21     Epoch time: 7.6042, Total time: 169.5864\n",
      "INFO     17:20:21     Evaluating...\n",
      "INFO     17:20:26     [25:  40]: eps=0.000823485447 CE=1.5999 Err=0.5696 Loss=2.0975 Robust_CE=2.0975 Verified_Err=0.7404 Time=0.0190\n",
      "INFO     17:20:26     Epoch 26, learning rate [0.0005]\n",
      "INFO     17:20:34     [26:  40]: eps=0.001049860492 CE=0.0000 grad_norm=4.2892 Loss=1.3461 Robust_CE=1.3461 Time=0.0785\n",
      "INFO     17:20:34     Epoch time: 7.7045, Total time: 177.2909\n",
      "INFO     17:20:34     Evaluating...\n",
      "INFO     17:20:39     [26:  40]: eps=0.001049860492 CE=1.5638 Err=0.5528 Loss=2.1177 Robust_CE=2.1177 Verified_Err=0.7564 Time=0.0191\n",
      "INFO     17:20:39     Epoch 27, learning rate [0.0005]\n",
      "INFO     17:20:46     [27:  40]: eps=0.001319982147 CE=0.0000 grad_norm=4.0355 Loss=1.3798 Robust_CE=1.3798 Time=0.0763\n",
      "INFO     17:20:46     Epoch time: 7.5209, Total time: 184.8117\n",
      "INFO     17:20:46     Evaluating...\n",
      "INFO     17:20:52     [27:  40]: eps=0.001319982147 CE=1.5559 Err=0.5584 Loss=2.1564 Robust_CE=2.1564 Verified_Err=0.7620 Time=0.0181\n",
      "INFO     17:20:52     Epoch 28, learning rate [0.0005]\n",
      "INFO     17:20:59     [28:  40]: eps=0.001639153402 CE=0.0000 grad_norm=4.2793 Loss=1.4251 Robust_CE=1.4251 Time=0.0782\n",
      "INFO     17:20:59     Epoch time: 7.5921, Total time: 192.4038\n",
      "INFO     17:20:59     Evaluating...\n",
      "INFO     17:21:04     [28:  40]: eps=0.001639153402 CE=1.5685 Err=0.5600 Loss=2.2116 Robust_CE=2.2116 Verified_Err=0.7796 Time=0.0184\n",
      "INFO     17:21:04     Epoch 29, learning rate [0.0005]\n",
      "INFO     17:21:12     [29:  40]: eps=0.002012980710 CE=0.0000 grad_norm=3.7828 Loss=1.4735 Robust_CE=1.4735 Time=0.0785\n",
      "INFO     17:21:12     Epoch time: 7.6004, Total time: 200.0042\n",
      "INFO     17:21:12     Evaluating...\n",
      "INFO     17:21:17     [29:  40]: eps=0.002012980710 CE=1.5552 Err=0.5536 Loss=2.2224 Robust_CE=2.2224 Verified_Err=0.7848 Time=0.0181\n",
      "INFO     17:21:17     Epoch 30, learning rate [0.0005]\n",
      "INFO     17:21:25     [30:  40]: eps=0.002447373985 CE=0.0000 grad_norm=3.3085 Loss=1.5154 Robust_CE=1.5154 Time=0.0782\n",
      "INFO     17:21:25     Epoch time: 7.5527, Total time: 207.5569\n",
      "INFO     17:21:25     Evaluating...\n",
      "INFO     17:21:30     [30:  40]: eps=0.002447373985 CE=1.5908 Err=0.5680 Loss=2.2550 Robust_CE=2.2550 Verified_Err=0.7992 Time=0.0197\n",
      "INFO     17:21:30     Epoch 31, learning rate [0.0005]\n",
      "INFO     17:21:38     [31:  40]: eps=0.002948546602 CE=0.0000 grad_norm=3.4820 Loss=1.5640 Robust_CE=1.5640 Time=0.0746\n",
      "INFO     17:21:38     Epoch time: 7.4928, Total time: 215.0497\n",
      "INFO     17:21:38     Evaluating...\n",
      "INFO     17:21:43     [31:  40]: eps=0.002948546602 CE=1.5638 Err=0.5524 Loss=2.2612 Robust_CE=2.2612 Verified_Err=0.8120 Time=0.0193\n",
      "INFO     17:21:43     Epoch 32, learning rate [0.0005]\n",
      "INFO     17:21:51     [32:  40]: eps=0.003523015399 CE=0.0000 grad_norm=3.1066 Loss=1.5981 Robust_CE=1.5981 Time=0.0839\n",
      "INFO     17:21:51     Epoch time: 7.9067, Total time: 222.9564\n",
      "INFO     17:21:51     Evaluating...\n",
      "INFO     17:21:56     [32:  40]: eps=0.003523015399 CE=1.5824 Err=0.5644 Loss=2.2855 Robust_CE=2.2855 Verified_Err=0.8024 Time=0.0191\n",
      "INFO     17:21:56     Epoch 33, learning rate [0.0005]\n",
      "INFO     17:22:04     [33:  40]: eps=0.004177600674 CE=0.0000 grad_norm=2.9553 Loss=1.6372 Robust_CE=1.6372 Time=0.0820\n",
      "INFO     17:22:04     Epoch time: 7.8693, Total time: 230.8257\n",
      "INFO     17:22:04     Evaluating...\n",
      "INFO     17:22:09     [33:  40]: eps=0.004177600674 CE=1.5855 Err=0.5680 Loss=2.3051 Robust_CE=2.3051 Verified_Err=0.8084 Time=0.0189\n",
      "INFO     17:22:09     Epoch 34, learning rate [0.0005]\n",
      "INFO     17:22:17     [34:  40]: eps=0.004904256785 CE=0.0000 grad_norm=2.7513 Loss=1.6777 Robust_CE=1.6777 Time=0.0804\n",
      "INFO     17:22:17     Epoch time: 7.8009, Total time: 238.6265\n",
      "INFO     17:22:17     Evaluating...\n",
      "INFO     17:22:23     [34:  40]: eps=0.004904256785 CE=1.5962 Err=0.5684 Loss=2.3285 Robust_CE=2.3285 Verified_Err=0.8064 Time=0.0193\n",
      "INFO     17:22:23     Epoch 35, learning rate [0.0005]\n",
      "INFO     17:22:30     [35:  40]: eps=0.005638976902 CE=0.0000 grad_norm=3.0152 Loss=1.7126 Robust_CE=1.7126 Time=0.0795\n",
      "INFO     17:22:30     Epoch time: 7.7124, Total time: 246.3389\n",
      "INFO     17:22:30     Evaluating...\n",
      "INFO     17:22:35     [35:  40]: eps=0.005638976902 CE=1.5986 Err=0.5720 Loss=2.3163 Robust_CE=2.3163 Verified_Err=0.8140 Time=0.0187\n",
      "INFO     17:22:35     Epoch 36, learning rate [0.0005]\n",
      "INFO     17:22:43     [36:  40]: eps=0.006373697020 CE=0.0000 grad_norm=2.6411 Loss=1.7383 Robust_CE=1.7383 Time=0.0774\n",
      "INFO     17:22:43     Epoch time: 7.4516, Total time: 253.7905\n",
      "INFO     17:22:43     Evaluating...\n",
      "INFO     17:22:49     [36:  40]: eps=0.006373697020 CE=1.6100 Err=0.5740 Loss=2.3642 Robust_CE=2.3642 Verified_Err=0.8076 Time=0.0207\n",
      "INFO     17:22:49     Epoch 37, learning rate [0.0005]\n",
      "INFO     17:22:58     [37:  40]: eps=0.007108417137 CE=0.0000 grad_norm=2.7195 Loss=1.7725 Robust_CE=1.7725 Time=0.0765\n",
      "INFO     17:22:58     Epoch time: 8.9128, Total time: 262.7034\n",
      "INFO     17:22:58     Evaluating...\n",
      "INFO     17:23:04     [37:  40]: eps=0.007108417137 CE=1.6011 Err=0.5728 Loss=2.3250 Robust_CE=2.3250 Verified_Err=0.7948 Time=0.0199\n",
      "INFO     17:23:04     Epoch 38, learning rate [0.0005]\n",
      "INFO     17:23:12     [38:  40]: eps=0.007843137255 CE=0.0000 grad_norm=2.6437 Loss=1.7916 Robust_CE=1.7916 Time=0.0748\n",
      "INFO     17:23:12     Epoch time: 7.8578, Total time: 270.5611\n",
      "INFO     17:23:12     Evaluating...\n",
      "INFO     17:23:17     [38:  40]: eps=0.007843137255 CE=1.6163 Err=0.5772 Loss=2.3707 Robust_CE=2.3707 Verified_Err=0.7984 Time=0.0211\n",
      "INFO     17:23:17     Epoch 39, learning rate [0.0005]\n",
      "INFO     17:23:25     [39:  40]: eps=0.008577857372 CE=0.0000 grad_norm=2.4855 Loss=1.8059 Robust_CE=1.8059 Time=0.0748\n",
      "INFO     17:23:25     Epoch time: 7.6455, Total time: 278.2066\n",
      "INFO     17:23:25     Evaluating...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO     17:23:30     [39:  40]: eps=0.008577857372 CE=1.6535 Err=0.5832 Loss=2.4296 Robust_CE=2.4296 Verified_Err=0.7876 Time=0.0198\n",
      "INFO     17:23:30     Epoch 40, learning rate [0.0005]\n",
      "INFO     17:23:38     [40:  40]: eps=0.009312577490 CE=0.0000 grad_norm=2.4444 Loss=1.8655 Robust_CE=1.8655 Time=0.0829\n",
      "INFO     17:23:38     Epoch time: 8.1071, Total time: 286.3137\n",
      "INFO     17:23:38     Evaluating...\n",
      "INFO     17:23:44     [40:  40]: eps=0.009312577490 CE=1.6638 Err=0.5900 Loss=2.4274 Robust_CE=2.4274 Verified_Err=0.7880 Time=0.0211\n",
      "INFO     17:23:44     Epoch 41, learning rate [0.0005]\n",
      "INFO     17:23:52     [41:  40]: eps=0.010047297608 CE=0.0000 grad_norm=2.6319 Loss=1.8904 Robust_CE=1.8904 Time=0.0851\n",
      "INFO     17:23:52     Epoch time: 8.4869, Total time: 294.8005\n",
      "INFO     17:23:52     Evaluating...\n",
      "INFO     17:23:58     [41:  40]: eps=0.010047297608 CE=1.6527 Err=0.5880 Loss=2.4239 Robust_CE=2.4239 Verified_Err=0.7772 Time=0.0216\n",
      "INFO     17:23:58     Epoch 42, learning rate [0.0005]\n",
      "INFO     17:24:06     [42:  40]: eps=0.010782017725 CE=0.0000 grad_norm=2.4963 Loss=1.9065 Robust_CE=1.9065 Time=0.0748\n",
      "INFO     17:24:06     Epoch time: 7.5431, Total time: 302.3437\n",
      "INFO     17:24:06     Evaluating...\n",
      "INFO     17:24:11     [42:  40]: eps=0.010782017725 CE=1.6663 Err=0.5924 Loss=2.4200 Robust_CE=2.4200 Verified_Err=0.7700 Time=0.0203\n",
      "INFO     17:24:11     Epoch 43, learning rate [0.0005]\n",
      "INFO     17:24:19     [43:  40]: eps=0.011516737843 CE=0.0000 grad_norm=2.1629 Loss=1.9512 Robust_CE=1.9512 Time=0.0859\n",
      "INFO     17:24:19     Epoch time: 8.0837, Total time: 310.4273\n",
      "INFO     17:24:19     Evaluating...\n",
      "INFO     17:24:25     [43:  40]: eps=0.011516737843 CE=1.6928 Err=0.5936 Loss=2.4404 Robust_CE=2.4404 Verified_Err=0.7752 Time=0.0204\n",
      "INFO     17:24:25     Epoch 44, learning rate [0.0005]\n",
      "INFO     17:24:32     [44:  40]: eps=0.012251457960 CE=0.0000 grad_norm=2.3641 Loss=1.9889 Robust_CE=1.9889 Time=0.0780\n",
      "INFO     17:24:32     Epoch time: 7.5328, Total time: 317.9601\n",
      "INFO     17:24:32     Evaluating...\n",
      "INFO     17:24:37     [44:  40]: eps=0.012251457960 CE=1.7198 Err=0.6044 Loss=2.4840 Robust_CE=2.4840 Verified_Err=0.7608 Time=0.0189\n",
      "INFO     17:24:37     Epoch 45, learning rate [0.0005]\n",
      "INFO     17:24:45     [45:  40]: eps=0.012986178078 CE=0.0000 grad_norm=2.1743 Loss=2.0239 Robust_CE=2.0239 Time=0.0843\n",
      "INFO     17:24:45     Epoch time: 7.8822, Total time: 325.8423\n",
      "INFO     17:24:45     Evaluating...\n",
      "INFO     17:24:51     [45:  40]: eps=0.012986178078 CE=1.7434 Err=0.6128 Loss=2.5121 Robust_CE=2.5121 Verified_Err=0.7612 Time=0.0187\n",
      "INFO     17:24:51     Epoch 46, learning rate [0.0005]\n",
      "INFO     17:25:02     [46:  40]: eps=0.013720898195 CE=0.0000 grad_norm=2.1454 Loss=2.0817 Robust_CE=2.0817 Time=0.1169\n",
      "INFO     17:25:02     Epoch time: 11.0816, Total time: 336.9239\n",
      "INFO     17:25:02     Evaluating...\n",
      "INFO     17:25:08     [46:  40]: eps=0.013720898195 CE=1.7945 Err=0.6192 Loss=2.6060 Robust_CE=2.6060 Verified_Err=0.7612 Time=0.0201\n",
      "INFO     17:25:08     Epoch 47, learning rate [0.0005]\n",
      "INFO     17:25:16     [47:  40]: eps=0.014455618313 CE=0.0000 grad_norm=2.5085 Loss=2.1132 Robust_CE=2.1132 Time=0.0844\n",
      "INFO     17:25:16     Epoch time: 8.2498, Total time: 345.1737\n",
      "INFO     17:25:16     Evaluating...\n",
      "INFO     17:25:21     [47:  40]: eps=0.014455618313 CE=1.7938 Err=0.6116 Loss=2.5906 Robust_CE=2.5906 Verified_Err=0.7588 Time=0.0183\n",
      "INFO     17:25:21     Epoch 48, learning rate [0.0005]\n",
      "INFO     17:25:29     [48:  40]: eps=0.015190338430 CE=0.0000 grad_norm=2.3561 Loss=2.1754 Robust_CE=2.1754 Time=0.0802\n",
      "INFO     17:25:29     Epoch time: 7.9693, Total time: 353.1430\n",
      "INFO     17:25:29     Evaluating...\n",
      "INFO     17:25:35     [48:  40]: eps=0.015190338430 CE=1.8618 Err=0.6408 Loss=2.6415 Robust_CE=2.6415 Verified_Err=0.7552 Time=0.0190\n",
      "INFO     17:25:35     Epoch 49, learning rate [0.0005]\n",
      "INFO     17:25:42     [49:  40]: eps=0.015925058548 CE=0.0000 grad_norm=2.4193 Loss=2.2099 Robust_CE=2.2099 Time=0.0760\n",
      "INFO     17:25:42     Epoch time: 7.3779, Total time: 360.5208\n",
      "INFO     17:25:42     Evaluating...\n",
      "INFO     17:25:47     [49:  40]: eps=0.015925058548 CE=1.8700 Err=0.6292 Loss=2.6910 Robust_CE=2.6910 Verified_Err=0.7504 Time=0.0189\n",
      "INFO     17:25:47     Epoch 50, learning rate [0.0005]\n",
      "INFO     17:25:55     [50:  40]: eps=0.016659778666 CE=0.0000 grad_norm=2.2287 Loss=2.2546 Robust_CE=2.2546 Time=0.0857\n",
      "INFO     17:25:55     Epoch time: 7.8688, Total time: 368.3896\n",
      "INFO     17:25:55     Evaluating...\n",
      "INFO     17:26:00     [50:  40]: eps=0.016659778666 CE=1.8698 Err=0.6256 Loss=2.6898 Robust_CE=2.6898 Verified_Err=0.7592 Time=0.0154\n",
      "INFO     17:26:00     Epoch 51, learning rate [0.0005]\n",
      "INFO     17:26:06     [51:  40]: eps=0.017394498783 CE=0.0000 grad_norm=2.1284 Loss=2.3245 Robust_CE=2.3245 Time=0.0528\n",
      "INFO     17:26:06     Epoch time: 5.9087, Total time: 374.2984\n",
      "INFO     17:26:06     Evaluating...\n",
      "INFO     17:26:10     [51:  40]: eps=0.017394498783 CE=1.9121 Err=0.6352 Loss=2.7276 Robust_CE=2.7276 Verified_Err=0.7576 Time=0.0133\n",
      "INFO     17:26:11     Epoch 52, learning rate [0.0005]\n",
      "INFO     17:26:16     [52:  40]: eps=0.018129218901 CE=0.0000 grad_norm=2.0753 Loss=2.3567 Robust_CE=2.3567 Time=0.0555\n",
      "INFO     17:26:16     Epoch time: 5.8804, Total time: 380.1788\n",
      "INFO     17:26:16     Evaluating...\n",
      "INFO     17:26:21     [52:  40]: eps=0.018129218901 CE=2.0305 Err=0.6372 Loss=2.8772 Robust_CE=2.8772 Verified_Err=0.7540 Time=0.0131\n",
      "INFO     17:26:21     Epoch 53, learning rate [0.0005]\n",
      "INFO     17:26:26     [53:  40]: eps=0.018863939018 CE=0.0000 grad_norm=2.4257 Loss=2.4307 Robust_CE=2.4307 Time=0.0564\n",
      "INFO     17:26:26     Epoch time: 5.8838, Total time: 386.0625\n",
      "INFO     17:26:26     Evaluating...\n",
      "INFO     17:26:31     [53:  40]: eps=0.018863939018 CE=1.9733 Err=0.6356 Loss=2.8469 Robust_CE=2.8469 Verified_Err=0.7560 Time=0.0133\n",
      "INFO     17:26:31     Epoch 54, learning rate [0.0005]\n",
      "INFO     17:26:37     [54:  40]: eps=0.019598659136 CE=0.0000 grad_norm=2.1407 Loss=2.4968 Robust_CE=2.4968 Time=0.0553\n",
      "INFO     17:26:37     Epoch time: 5.9624, Total time: 392.0249\n",
      "INFO     17:26:37     Evaluating...\n",
      "INFO     17:26:41     [54:  40]: eps=0.019598659136 CE=2.0583 Err=0.6344 Loss=2.9624 Robust_CE=2.9624 Verified_Err=0.7536 Time=0.0141\n",
      "INFO     17:26:41     Epoch 55, learning rate [0.0005]\n",
      "INFO     17:26:47     [55:  40]: eps=0.020333379253 CE=0.0000 grad_norm=2.3588 Loss=2.5720 Robust_CE=2.5720 Time=0.0548\n",
      "INFO     17:26:47     Epoch time: 5.8577, Total time: 397.8826\n",
      "INFO     17:26:47     Evaluating...\n",
      "INFO     17:26:51     [55:  40]: eps=0.020333379253 CE=2.0729 Err=0.6376 Loss=2.9862 Robust_CE=2.9862 Verified_Err=0.7560 Time=0.0137\n",
      "INFO     17:26:51     Epoch 56, learning rate [0.0005]\n",
      "INFO     17:26:57     [56:  40]: eps=0.021068099371 CE=0.0000 grad_norm=2.4238 Loss=2.6520 Robust_CE=2.6520 Time=0.0559\n",
      "INFO     17:26:57     Epoch time: 6.0333, Total time: 403.9159\n",
      "INFO     17:26:57     Evaluating...\n",
      "INFO     17:27:02     [56:  40]: eps=0.021068099371 CE=2.2058 Err=0.6480 Loss=3.1434 Robust_CE=3.1434 Verified_Err=0.7608 Time=0.0129\n",
      "INFO     17:27:02     Epoch 57, learning rate [0.0005]\n",
      "INFO     17:27:08     [57:  40]: eps=0.021802819488 CE=0.0000 grad_norm=2.2945 Loss=2.7446 Robust_CE=2.7446 Time=0.0547\n",
      "INFO     17:27:08     Epoch time: 6.2307, Total time: 410.1467\n",
      "INFO     17:27:08     Evaluating...\n",
      "INFO     17:27:12     [57:  40]: eps=0.021802819488 CE=2.1932 Err=0.6396 Loss=3.1462 Robust_CE=3.1462 Verified_Err=0.7584 Time=0.0130\n",
      "INFO     17:27:12     Epoch 58, learning rate [0.0005]\n",
      "INFO     17:27:19     [58:  40]: eps=0.022537539606 CE=0.0000 grad_norm=2.7614 Loss=2.8532 Robust_CE=2.8532 Time=0.0583\n",
      "INFO     17:27:19     Epoch time: 6.0841, Total time: 416.2307\n",
      "INFO     17:27:19     Evaluating...\n",
      "INFO     17:27:23     [58:  40]: eps=0.022537539606 CE=2.2300 Err=0.6400 Loss=3.2333 Robust_CE=3.2333 Verified_Err=0.7644 Time=0.0136\n",
      "INFO     17:27:23     Epoch 59, learning rate [0.0005]\n",
      "INFO     17:27:29     [59:  40]: eps=0.023272259724 CE=0.0000 grad_norm=2.1934 Loss=2.9898 Robust_CE=2.9898 Time=0.0552\n",
      "INFO     17:27:29     Epoch time: 5.8397, Total time: 422.0704\n",
      "INFO     17:27:29     Evaluating...\n",
      "INFO     17:27:33     [59:  40]: eps=0.023272259724 CE=2.5239 Err=0.6804 Loss=3.5742 Robust_CE=3.5742 Verified_Err=0.7752 Time=0.0130\n",
      "INFO     17:27:33     Epoch 60, learning rate [0.0005]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO     17:27:39     [60:  40]: eps=0.024006979841 CE=0.0000 grad_norm=2.3582 Loss=3.0521 Robust_CE=3.0521 Time=0.0549\n",
      "INFO     17:27:39     Epoch time: 5.8974, Total time: 427.9678\n",
      "INFO     17:27:39     Evaluating...\n",
      "INFO     17:27:43     [60:  40]: eps=0.024006979841 CE=2.4422 Err=0.6460 Loss=3.5363 Robust_CE=3.5363 Verified_Err=0.7664 Time=0.0131\n",
      "INFO     17:27:43     Epoch 61, learning rate [0.0005]\n",
      "INFO     17:27:49     [61:  40]: eps=0.024741699959 CE=0.0000 grad_norm=2.4560 Loss=3.1521 Robust_CE=3.1521 Time=0.0552\n",
      "INFO     17:27:49     Epoch time: 5.8529, Total time: 433.8207\n",
      "INFO     17:27:49     Evaluating...\n",
      "INFO     17:27:53     [61:  40]: eps=0.024741699959 CE=2.4828 Err=0.6532 Loss=3.5680 Robust_CE=3.5680 Verified_Err=0.7668 Time=0.0129\n",
      "INFO     17:27:53     Epoch 62, learning rate [0.0005]\n",
      "INFO     17:28:00     [62:  40]: eps=0.025476420076 CE=0.0000 grad_norm=2.6698 Loss=3.3487 Robust_CE=3.3487 Time=0.0753\n",
      "INFO     17:28:00     Epoch time: 6.7638, Total time: 440.5845\n",
      "INFO     17:28:00     Evaluating...\n",
      "INFO     17:28:04     [62:  40]: eps=0.025476420076 CE=2.5413 Err=0.6560 Loss=3.7276 Robust_CE=3.7276 Verified_Err=0.7684 Time=0.0136\n",
      "INFO     17:28:04     Epoch 63, learning rate [0.0005]\n",
      "INFO     17:28:11     [63:  40]: eps=0.026211140194 CE=0.0000 grad_norm=2.7842 Loss=3.5804 Robust_CE=3.5804 Time=0.0589\n",
      "INFO     17:28:11     Epoch time: 6.2752, Total time: 446.8597\n",
      "INFO     17:28:11     Evaluating...\n",
      "INFO     17:28:15     [63:  40]: eps=0.026211140194 CE=2.9110 Err=0.6772 Loss=4.1405 Robust_CE=4.1405 Verified_Err=0.7816 Time=0.0135\n",
      "INFO     17:28:15     Epoch 64, learning rate [0.0005]\n",
      "INFO     17:28:22     [64:  40]: eps=0.026945860311 CE=0.0000 grad_norm=2.9996 Loss=3.8096 Robust_CE=3.8096 Time=0.0682\n",
      "INFO     17:28:22     Epoch time: 6.7225, Total time: 453.5822\n",
      "INFO     17:28:22     Evaluating...\n",
      "INFO     17:28:26     [64:  40]: eps=0.026945860311 CE=2.7210 Err=0.6624 Loss=3.9231 Robust_CE=3.9231 Verified_Err=0.7668 Time=0.0141\n",
      "INFO     17:28:26     Epoch 65, learning rate [0.0005]\n",
      "INFO     17:28:33     [65:  40]: eps=0.027680580429 CE=0.0000 grad_norm=2.5132 Loss=4.0089 Robust_CE=4.0089 Time=0.0623\n",
      "INFO     17:28:33     Epoch time: 6.8567, Total time: 460.4389\n",
      "INFO     17:28:33     Evaluating...\n",
      "INFO     17:28:37     [65:  40]: eps=0.027680580429 CE=3.4219 Err=0.7024 Loss=4.7864 Robust_CE=4.7864 Verified_Err=0.7808 Time=0.0127\n",
      "INFO     17:28:37     Epoch 66, learning rate [0.0005]\n",
      "INFO     17:28:43     [66:  40]: eps=0.028415300546 CE=0.0000 grad_norm=2.5720 Loss=4.3105 Robust_CE=4.3105 Time=0.0544\n",
      "INFO     17:28:43     Epoch time: 5.8981, Total time: 466.3370\n",
      "INFO     17:28:43     Evaluating...\n",
      "INFO     17:28:47     [66:  40]: eps=0.028415300546 CE=3.9930 Err=0.7208 Loss=5.4441 Robust_CE=5.4441 Verified_Err=0.8048 Time=0.0131\n",
      "INFO     17:28:47     Epoch 67, learning rate [0.0005]\n",
      "INFO     17:28:53     [67:  40]: eps=0.029150020664 CE=0.0000 grad_norm=2.5866 Loss=5.2536 Robust_CE=5.2536 Time=0.0539\n",
      "INFO     17:28:53     Epoch time: 5.7725, Total time: 472.1095\n",
      "INFO     17:28:53     Evaluating...\n",
      "INFO     17:28:57     [67:  40]: eps=0.029150020664 CE=3.7100 Err=0.7256 Loss=5.0189 Robust_CE=5.0189 Verified_Err=0.8084 Time=0.0134\n",
      "INFO     17:28:57     Epoch 68, learning rate [0.0005]\n",
      "INFO     17:29:04     [68:  40]: eps=0.029884740782 CE=0.0000 grad_norm=3.0267 Loss=5.5486 Robust_CE=5.5486 Time=0.0624\n",
      "INFO     17:29:04     Epoch time: 6.4055, Total time: 478.5150\n",
      "INFO     17:29:04     Evaluating...\n",
      "INFO     17:29:09     [68:  40]: eps=0.029884740782 CE=4.2350 Err=0.7548 Loss=5.7312 Robust_CE=5.7312 Verified_Err=0.8224 Time=0.0161\n",
      "INFO     17:29:09     Epoch 69, learning rate [0.0005]\n",
      "INFO     17:29:15     [69:  40]: eps=0.030619460899 CE=0.0000 grad_norm=3.0566 Loss=6.5729 Robust_CE=6.5729 Time=0.0618\n",
      "INFO     17:29:15     Epoch time: 6.4377, Total time: 484.9527\n",
      "INFO     17:29:15     Evaluating...\n",
      "INFO     17:29:20     [69:  40]: eps=0.030619460899 CE=5.4722 Err=0.8348 Loss=6.8854 Robust_CE=6.8854 Verified_Err=0.8632 Time=0.0134\n",
      "INFO     17:29:20     Epoch 70, learning rate [0.0005]\n",
      "INFO     17:29:26     [70:  40]: eps=0.031354181017 CE=0.0000 grad_norm=1.9284 Loss=7.4421 Robust_CE=7.4421 Time=0.0573\n",
      "INFO     17:29:26     Epoch time: 6.0147, Total time: 490.9674\n",
      "INFO     17:29:26     Evaluating...\n",
      "INFO     17:29:30     [70:  40]: eps=0.031354181017 CE=6.8915 Err=0.8472 Loss=8.4546 Robust_CE=8.4546 Verified_Err=0.8748 Time=0.0164\n",
      "INFO     17:29:30     Epoch 71, learning rate [5e-05]\n",
      "INFO     17:29:35     [71:  40]: eps=0.031372549020 CE=0.0000 grad_norm=20.0562 Loss=5.2941 Robust_CE=5.2941 Time=0.0252\n",
      "INFO     17:29:35     Epoch time: 4.8270, Total time: 495.7944\n",
      "INFO     17:29:35     Evaluating...\n",
      "INFO     17:29:39     [71:  40]: eps=0.031372549020 CE=2.1772 Err=0.7176 Loss=2.9510 Robust_CE=2.9510 Verified_Err=0.8384 Time=0.0130\n",
      "INFO     17:29:39     Epoch 72, learning rate [5e-05]\n",
      "INFO     17:29:44     [72:  40]: eps=0.031372549020 CE=0.0000 grad_norm=3.8613 Loss=2.4166 Robust_CE=2.4166 Time=0.0243\n",
      "INFO     17:29:44     Epoch time: 4.6151, Total time: 500.4096\n",
      "INFO     17:29:44     Evaluating...\n",
      "INFO     17:29:48     [72:  40]: eps=0.031372549020 CE=1.9018 Err=0.6808 Loss=2.3581 Robust_CE=2.3581 Verified_Err=0.8580 Time=0.0129\n",
      "INFO     17:29:48     Epoch 73, learning rate [5e-05]\n",
      "INFO     17:29:53     [73:  40]: eps=0.031372549020 CE=0.0000 grad_norm=2.1694 Loss=2.2071 Robust_CE=2.2071 Time=0.0247\n",
      "INFO     17:29:53     Epoch time: 4.7567, Total time: 505.1663\n",
      "INFO     17:29:53     Evaluating...\n",
      "INFO     17:29:57     [73:  40]: eps=0.031372549020 CE=1.8903 Err=0.6864 Loss=2.2884 Robust_CE=2.2884 Verified_Err=0.8488 Time=0.0125\n",
      "INFO     17:29:57     Epoch 74, learning rate [5e-05]\n",
      "INFO     17:30:02     [74:  40]: eps=0.031372549020 CE=0.0000 grad_norm=2.4370 Loss=2.1623 Robust_CE=2.1623 Time=0.0245\n",
      "INFO     17:30:02     Epoch time: 4.6425, Total time: 509.8088\n",
      "INFO     17:30:02     Evaluating...\n",
      "INFO     17:30:06     [74:  40]: eps=0.031372549020 CE=1.8801 Err=0.6876 Loss=2.2541 Robust_CE=2.2541 Verified_Err=0.8400 Time=0.0127\n",
      "INFO     17:30:06     Epoch 75, learning rate [5e-05]\n",
      "INFO     17:30:11     [75:  40]: eps=0.031372549020 CE=0.0000 grad_norm=2.2067 Loss=2.1330 Robust_CE=2.1330 Time=0.0264\n",
      "INFO     17:30:11     Epoch time: 4.9155, Total time: 514.7242\n",
      "INFO     17:30:11     Evaluating...\n",
      "INFO     17:30:15     [75:  40]: eps=0.031372549020 CE=1.8730 Err=0.6824 Loss=2.2292 Robust_CE=2.2292 Verified_Err=0.8328 Time=0.0131\n",
      "INFO     17:30:15     Epoch 76, learning rate [5e-05]\n",
      "INFO     17:30:20     [76:  40]: eps=0.031372549020 CE=0.0000 grad_norm=2.1509 Loss=2.1117 Robust_CE=2.1117 Time=0.0245\n",
      "INFO     17:30:20     Epoch time: 4.6570, Total time: 519.3812\n",
      "INFO     17:30:20     Evaluating...\n",
      "INFO     17:30:24     [76:  40]: eps=0.031372549020 CE=1.8657 Err=0.6792 Loss=2.2156 Robust_CE=2.2156 Verified_Err=0.8312 Time=0.0128\n",
      "INFO     17:30:24     Epoch 77, learning rate [5e-05]\n",
      "INFO     17:30:29     [77:  40]: eps=0.031372549020 CE=0.0000 grad_norm=2.0907 Loss=2.0952 Robust_CE=2.0952 Time=0.0241\n",
      "INFO     17:30:29     Epoch time: 4.6704, Total time: 524.0516\n",
      "INFO     17:30:29     Evaluating...\n",
      "INFO     17:30:33     [77:  40]: eps=0.031372549020 CE=1.8630 Err=0.6796 Loss=2.2012 Robust_CE=2.2012 Verified_Err=0.8300 Time=0.0126\n",
      "INFO     17:30:33     Epoch 78, learning rate [5e-05]\n",
      "INFO     17:30:38     [78:  40]: eps=0.031372549020 CE=0.0000 grad_norm=2.2348 Loss=2.0827 Robust_CE=2.0827 Time=0.0257\n",
      "INFO     17:30:38     Epoch time: 4.8161, Total time: 528.8677\n",
      "INFO     17:30:38     Evaluating...\n",
      "INFO     17:30:42     [78:  40]: eps=0.031372549020 CE=1.8605 Err=0.6788 Loss=2.1901 Robust_CE=2.1901 Verified_Err=0.8216 Time=0.0159\n",
      "INFO     17:30:42     Epoch 79, learning rate [5e-05]\n",
      "INFO     17:30:47     [79:  40]: eps=0.031372549020 CE=0.0000 grad_norm=2.2200 Loss=2.0730 Robust_CE=2.0730 Time=0.0255\n",
      "INFO     17:30:47     Epoch time: 4.9993, Total time: 533.8671\n",
      "INFO     17:30:47     Evaluating...\n",
      "INFO     17:30:52     [79:  40]: eps=0.031372549020 CE=1.8575 Err=0.6820 Loss=2.1811 Robust_CE=2.1811 Verified_Err=0.8180 Time=0.0134\n",
      "INFO     17:30:52     Epoch 80, learning rate [5e-05]\n",
      "INFO     17:30:56     [80:  40]: eps=0.031372549020 CE=0.0000 grad_norm=2.4090 Loss=2.0630 Robust_CE=2.0630 Time=0.0250\n",
      "INFO     17:30:56     Epoch time: 4.6986, Total time: 538.5657\n",
      "INFO     17:30:56     Evaluating...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO     17:31:01     [80:  40]: eps=0.031372549020 CE=1.8580 Err=0.6848 Loss=2.1728 Robust_CE=2.1728 Verified_Err=0.8128 Time=0.0133\n",
      "INFO     17:31:01     Epoch 81, learning rate [5e-05]\n",
      "INFO     17:31:05     [81:  40]: eps=0.031372549020 CE=0.0000 grad_norm=2.1078 Loss=2.0550 Robust_CE=2.0550 Time=0.0237\n",
      "INFO     17:31:05     Epoch time: 4.6103, Total time: 543.1760\n",
      "INFO     17:31:05     Evaluating...\n",
      "INFO     17:31:09     [81:  40]: eps=0.031372549020 CE=1.8574 Err=0.6816 Loss=2.1630 Robust_CE=2.1630 Verified_Err=0.8116 Time=0.0139\n",
      "INFO     17:31:09     Epoch 82, learning rate [5e-05]\n",
      "INFO     17:31:14     [82:  40]: eps=0.031372549020 CE=0.0000 grad_norm=2.1297 Loss=2.0480 Robust_CE=2.0480 Time=0.0254\n",
      "INFO     17:31:14     Epoch time: 4.8215, Total time: 547.9975\n",
      "INFO     17:31:14     Evaluating...\n",
      "INFO     17:31:19     [82:  40]: eps=0.031372549020 CE=1.8528 Err=0.6804 Loss=2.1592 Robust_CE=2.1592 Verified_Err=0.8064 Time=0.0183\n",
      "INFO     17:31:19     Epoch 83, learning rate [5e-05]\n",
      "INFO     17:31:25     [83:  40]: eps=0.031372549020 CE=0.0000 grad_norm=2.1209 Loss=2.0416 Robust_CE=2.0416 Time=0.0337\n",
      "INFO     17:31:25     Epoch time: 5.4017, Total time: 553.3992\n",
      "INFO     17:31:25     Evaluating...\n",
      "INFO     17:31:29     [83:  40]: eps=0.031372549020 CE=1.8496 Err=0.6800 Loss=2.1540 Robust_CE=2.1540 Verified_Err=0.8096 Time=0.0146\n",
      "INFO     17:31:29     Epoch 84, learning rate [5e-05]\n",
      "INFO     17:31:35     [84:  40]: eps=0.031372549020 CE=0.0000 grad_norm=2.1240 Loss=2.0354 Robust_CE=2.0354 Time=0.0315\n",
      "INFO     17:31:35     Epoch time: 5.3404, Total time: 558.7396\n",
      "INFO     17:31:35     Evaluating...\n",
      "INFO     17:31:39     [84:  40]: eps=0.031372549020 CE=1.8468 Err=0.6780 Loss=2.1511 Robust_CE=2.1511 Verified_Err=0.8068 Time=0.0127\n",
      "INFO     17:31:39     Epoch 85, learning rate [5e-05]\n",
      "INFO     17:31:44     [85:  40]: eps=0.031372549020 CE=0.0000 grad_norm=2.1505 Loss=2.0300 Robust_CE=2.0300 Time=0.0288\n",
      "INFO     17:31:44     Epoch time: 4.9286, Total time: 563.6682\n",
      "INFO     17:31:44     Evaluating...\n",
      "INFO     17:31:48     [85:  40]: eps=0.031372549020 CE=1.8468 Err=0.6780 Loss=2.1456 Robust_CE=2.1456 Verified_Err=0.8040 Time=0.0130\n",
      "INFO     17:31:48     Epoch 86, learning rate [5e-06]\n",
      "INFO     17:31:53     [86:  40]: eps=0.031372549020 CE=0.0000 grad_norm=1.9893 Loss=2.0248 Robust_CE=2.0248 Time=0.0247\n",
      "INFO     17:31:53     Epoch time: 4.7356, Total time: 568.4038\n",
      "INFO     17:31:53     Evaluating...\n",
      "INFO     17:31:57     [86:  40]: eps=0.031372549020 CE=1.8464 Err=0.6768 Loss=2.1440 Robust_CE=2.1440 Verified_Err=0.8052 Time=0.0128\n",
      "INFO     17:31:57     Epoch 87, learning rate [5e-06]\n",
      "INFO     17:32:02     [87:  40]: eps=0.031372549020 CE=0.0000 grad_norm=2.0716 Loss=2.0237 Robust_CE=2.0237 Time=0.0334\n",
      "INFO     17:32:02     Epoch time: 5.1701, Total time: 573.5739\n",
      "INFO     17:32:02     Evaluating...\n",
      "INFO     17:32:07     [87:  40]: eps=0.031372549020 CE=1.8455 Err=0.6744 Loss=2.1433 Robust_CE=2.1433 Verified_Err=0.8044 Time=0.0136\n",
      "INFO     17:32:07     Epoch 88, learning rate [5e-06]\n",
      "INFO     17:32:12     [88:  40]: eps=0.031372549020 CE=0.0000 grad_norm=1.9498 Loss=2.0231 Robust_CE=2.0231 Time=0.0257\n",
      "INFO     17:32:12     Epoch time: 4.9332, Total time: 578.5071\n",
      "INFO     17:32:12     Evaluating...\n",
      "INFO     17:32:16     [88:  40]: eps=0.031372549020 CE=1.8453 Err=0.6736 Loss=2.1426 Robust_CE=2.1426 Verified_Err=0.8048 Time=0.0131\n",
      "INFO     17:32:16     Epoch 89, learning rate [5e-06]\n",
      "INFO     17:32:21     [89:  40]: eps=0.031372549020 CE=0.0000 grad_norm=1.8433 Loss=2.0225 Robust_CE=2.0225 Time=0.0301\n",
      "INFO     17:32:21     Epoch time: 5.0552, Total time: 583.5623\n",
      "INFO     17:32:21     Evaluating...\n",
      "INFO     17:32:26     [89:  40]: eps=0.031372549020 CE=1.8453 Err=0.6720 Loss=2.1419 Robust_CE=2.1419 Verified_Err=0.8040 Time=0.0130\n",
      "INFO     17:32:26     Epoch 90, learning rate [5e-06]\n",
      "INFO     17:32:31     [90:  40]: eps=0.031372549020 CE=0.0000 grad_norm=2.0532 Loss=2.0219 Robust_CE=2.0219 Time=0.0235\n",
      "INFO     17:32:31     Epoch time: 4.6338, Total time: 588.1961\n",
      "INFO     17:32:31     Evaluating...\n",
      "INFO     17:32:35     [90:  40]: eps=0.031372549020 CE=1.8447 Err=0.6724 Loss=2.1418 Robust_CE=2.1418 Verified_Err=0.8036 Time=0.0159\n",
      "INFO     17:32:35     Epoch 91, learning rate [5e-06]\n",
      "INFO     17:32:40     [91:  40]: eps=0.031372549020 CE=0.0000 grad_norm=2.1597 Loss=2.0214 Robust_CE=2.0214 Time=0.0331\n",
      "INFO     17:32:40     Epoch time: 5.3572, Total time: 593.5533\n",
      "INFO     17:32:40     Evaluating...\n",
      "INFO     17:32:45     [91:  40]: eps=0.031372549020 CE=1.8443 Err=0.6712 Loss=2.1412 Robust_CE=2.1412 Verified_Err=0.8024 Time=0.0131\n",
      "INFO     17:32:45     Epoch 92, learning rate [5e-06]\n",
      "INFO     17:32:50     [92:  40]: eps=0.031372549020 CE=0.0000 grad_norm=2.0602 Loss=2.0209 Robust_CE=2.0209 Time=0.0251\n",
      "INFO     17:32:50     Epoch time: 4.8018, Total time: 598.3551\n",
      "INFO     17:32:50     Evaluating...\n",
      "INFO     17:32:54     [92:  40]: eps=0.031372549020 CE=1.8443 Err=0.6704 Loss=2.1405 Robust_CE=2.1405 Verified_Err=0.8020 Time=0.0177\n",
      "INFO     17:32:54     Epoch 93, learning rate [5e-06]\n",
      "INFO     17:33:00     [93:  40]: eps=0.031372549020 CE=0.0000 grad_norm=2.0797 Loss=2.0204 Robust_CE=2.0204 Time=0.0254\n",
      "INFO     17:33:00     Epoch time: 5.2431, Total time: 603.5982\n",
      "INFO     17:33:00     Evaluating...\n",
      "INFO     17:33:04     [93:  40]: eps=0.031372549020 CE=1.8438 Err=0.6700 Loss=2.1402 Robust_CE=2.1402 Verified_Err=0.8032 Time=0.0133\n",
      "INFO     17:33:04     Epoch 94, learning rate [5e-06]\n",
      "INFO     17:33:10     [94:  40]: eps=0.031372549020 CE=0.0000 grad_norm=2.0388 Loss=2.0198 Robust_CE=2.0198 Time=0.0324\n",
      "INFO     17:33:10     Epoch time: 5.4132, Total time: 609.0114\n",
      "INFO     17:33:10     Evaluating...\n",
      "INFO     17:33:14     [94:  40]: eps=0.031372549020 CE=1.8436 Err=0.6708 Loss=2.1396 Robust_CE=2.1396 Verified_Err=0.8036 Time=0.0132\n",
      "INFO     17:33:14     Epoch 95, learning rate [5e-06]\n",
      "INFO     17:33:19     [95:  40]: eps=0.031372549020 CE=0.0000 grad_norm=2.1036 Loss=2.0193 Robust_CE=2.0193 Time=0.0349\n",
      "INFO     17:33:19     Epoch time: 5.2721, Total time: 614.2835\n",
      "INFO     17:33:19     Evaluating...\n",
      "INFO     17:33:24     [95:  40]: eps=0.031372549020 CE=1.8432 Err=0.6696 Loss=2.1391 Robust_CE=2.1391 Verified_Err=0.8040 Time=0.0156\n",
      "INFO     17:33:24     Epoch 96, learning rate [5e-06]\n",
      "INFO     17:33:29     [96:  40]: eps=0.031372549020 CE=0.0000 grad_norm=1.9738 Loss=2.0188 Robust_CE=2.0188 Time=0.0248\n",
      "INFO     17:33:29     Epoch time: 4.9197, Total time: 619.2032\n",
      "INFO     17:33:29     Evaluating...\n",
      "INFO     17:33:33     [96:  40]: eps=0.031372549020 CE=1.8428 Err=0.6700 Loss=2.1384 Robust_CE=2.1384 Verified_Err=0.8032 Time=0.0132\n",
      "INFO     17:33:33     Epoch 97, learning rate [5e-06]\n",
      "INFO     17:33:38     [97:  40]: eps=0.031372549020 CE=0.0000 grad_norm=2.0834 Loss=2.0183 Robust_CE=2.0183 Time=0.0256\n",
      "INFO     17:33:38     Epoch time: 4.8173, Total time: 624.0205\n",
      "INFO     17:33:38     Evaluating...\n",
      "INFO     17:33:42     [97:  40]: eps=0.031372549020 CE=1.8425 Err=0.6696 Loss=2.1382 Robust_CE=2.1382 Verified_Err=0.8040 Time=0.0129\n",
      "INFO     17:33:42     Epoch 98, learning rate [5e-06]\n",
      "INFO     17:33:47     [98:  40]: eps=0.031372549020 CE=0.0000 grad_norm=2.0694 Loss=2.0179 Robust_CE=2.0179 Time=0.0240\n",
      "INFO     17:33:47     Epoch time: 4.7468, Total time: 628.7673\n",
      "INFO     17:33:47     Evaluating...\n",
      "INFO     17:33:51     [98:  40]: eps=0.031372549020 CE=1.8418 Err=0.6692 Loss=2.1380 Robust_CE=2.1380 Verified_Err=0.8024 Time=0.0128\n",
      "INFO     17:33:51     Epoch 99, learning rate [5e-06]\n",
      "INFO     17:33:56     [99:  40]: eps=0.031372549020 CE=0.0000 grad_norm=2.1234 Loss=2.0174 Robust_CE=2.0174 Time=0.0266\n",
      "INFO     17:33:56     Epoch time: 4.9218, Total time: 633.6890\n",
      "INFO     17:33:56     Evaluating...\n",
      "INFO     17:34:01     [99:  40]: eps=0.031372549020 CE=1.8418 Err=0.6684 Loss=2.1374 Robust_CE=2.1374 Verified_Err=0.8040 Time=0.0130\n",
      "INFO     17:34:01     Epoch 100, learning rate [5e-06]\n",
      "INFO     17:34:05     [100:  40]: eps=0.031372549020 CE=0.0000 grad_norm=2.1656 Loss=2.0168 Robust_CE=2.0168 Time=0.0242\n",
      "INFO     17:34:05     Epoch time: 4.7250, Total time: 638.4140\n",
      "INFO     17:34:05     Evaluating...\n",
      "INFO     17:34:10     [100:  40]: eps=0.031372549020 CE=1.8410 Err=0.6692 Loss=2.1375 Robust_CE=2.1375 Verified_Err=0.8028 Time=0.0125\n"
     ]
    }
   ],
   "source": [
    "! python auto_LiRPA/examples/vision/cifar_training.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "914176d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.3676\n",
      "Test accuracy: 0.3308\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "def get_acc(model, x, y):\n",
    "    _, predictions = torch.max(model(torch.from_numpy(x)), 1)\n",
    "    acc = np.mean(predictions.detach().numpy() == np.argmax(y, axis=1))\n",
    "    return acc\n",
    "\n",
    "model = cifar_model()\n",
    "model.load_state_dict(torch.load(\"models/cifar10_crown_ibp.pth\", map_location=torch.device('cpu'))['state_dict'])\n",
    "model.eval()\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "print(\"Train accuracy:\", get_acc(model, x_target_train, y_target_train))\n",
    "print(\"Test accuracy:\", get_acc(model, x_target_test, y_target_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a049c7c1",
   "metadata": {},
   "source": [
    "### 4. Generate Adversarial Examples using HopSkipJump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b921a18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca8198f1a2464e538ccc81dfae163217",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HopSkipJump:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1d7f4aed3274a3ba18d221f1835a243",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HopSkipJump:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from art.attacks.evasion import HopSkipJump\n",
    "from art.estimators.classification.pytorch import PyTorchClassifier\n",
    "from numpy.random import choice\n",
    "\n",
    "art_classifier=PyTorchClassifier(\n",
    "    model=model,\n",
    "    clip_values=(_min, _max),\n",
    "    loss=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    input_shape=(3, 32, 32),\n",
    "    nb_classes=10,\n",
    ")\n",
    "attack = HopSkipJump(classifier=art_classifier, norm=2, targeted=False)\n",
    "\n",
    "train_idx = choice(len(x_target_train), 100)\n",
    "y_adv_train = y_target_train[train_idx]\n",
    "x_adv_train = attack.generate(x=x_target_train[train_idx], y=y_adv_train)\n",
    "\n",
    "test_idx = choice(len(x_target_test), 100)\n",
    "y_adv_test = y_target_test[test_idx]\n",
    "x_adv_test = attack.generate(x=x_target_test[test_idx], y=y_adv_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64326b2",
   "metadata": {},
   "source": [
    "### 5. Measure the Distribution of Distance to Decision Boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e2a4898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                0\n",
      "count  100.000000\n",
      "mean     1.024536\n",
      "std      0.986315\n",
      "min      0.000000\n",
      "25%      0.106697\n",
      "50%      0.771034\n",
      "75%      1.634235\n",
      "max      3.761666\n",
      "                0\n",
      "count  100.000000\n",
      "mean     1.229705\n",
      "std      1.236906\n",
      "min      0.000000\n",
      "25%      0.204892\n",
      "50%      0.917901\n",
      "75%      1.917299\n",
      "max      6.522482\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def dist(x, x_adv):\n",
    "    return np.linalg.norm(np.reshape(x_adv - x, [-1]))\n",
    "\n",
    "dist_train = [dist(x, xa) for (x, xa) in zip(x_target_train[train_idx], x_adv_train)]\n",
    "dist_test = [dist(x, xa) for (x, xa) in zip(x_target_test[test_idx], x_adv_test)]\n",
    "print(pd.DataFrame(np.array(dist_train)).describe())\n",
    "print(pd.DataFrame(np.array(dist_test)).describe())\n",
    "\n",
    "bins = [0, 0.25, 0.5, 0.75, 1.0, 1.25, 1.5, 1.75, 2, 2.25, 2.5, 2.75, 3, 3.25]\n",
    "\n",
    "plt.hist(dist_train, bins, alpha=0.5, label='member')\n",
    "plt.hist(dist_test, bins, alpha=0.5, label='non-member')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title(f'{dataset} - {algorithm}')\n",
    "plt.xlabel(\"Distance to Decision Boundary in L2 Norm\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba80423",
   "metadata": {},
   "source": [
    "### 6. Measure the Advantage of An Adversary in Label-Only Membership Inference Attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed0254bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.   0.01 0.05 0.05 0.07 0.07 0.08 0.08 0.11 0.11 0.14 0.14 0.16 0.16\n",
      " 0.17 0.17 0.18 0.18 0.21 0.21 0.27 0.27 0.3  0.3  0.31 0.31 0.32 0.32\n",
      " 0.35 0.35 0.38 0.38 0.39 0.39 0.4  0.4  0.43 0.43 0.45 0.45 0.46 0.46\n",
      " 0.47 0.47 0.48 0.48 0.51 0.51 0.52 0.52 0.54 0.54 0.56 0.56 0.58 0.58\n",
      " 0.59 0.59 0.62 0.62 0.64 0.64 0.65 0.65 0.66 0.66 0.7  0.7  0.71 0.71\n",
      " 0.73 0.73 0.75 0.75 0.79 0.79 0.8  0.8  0.83 0.83 0.85 0.85 0.87 0.87\n",
      " 0.88 1.  ]\n",
      "[0.   0.   0.   0.01 0.01 0.02 0.02 0.06 0.06 0.09 0.09 0.15 0.15 0.16\n",
      " 0.16 0.19 0.19 0.2  0.2  0.21 0.21 0.22 0.22 0.23 0.23 0.24 0.24 0.27\n",
      " 0.27 0.28 0.28 0.31 0.31 0.32 0.32 0.36 0.36 0.37 0.37 0.38 0.38 0.41\n",
      " 0.41 0.43 0.43 0.45 0.45 0.47 0.47 0.48 0.48 0.51 0.51 0.54 0.54 0.55\n",
      " 0.55 0.59 0.59 0.61 0.61 0.63 0.63 0.65 0.65 0.67 0.67 0.68 0.68 0.7\n",
      " 0.7  0.71 0.71 0.72 0.72 0.73 0.73 0.74 0.74 0.76 0.76 0.79 0.79 0.8\n",
      " 0.8  1.  ]\n",
      "[7.5224819e+00 6.5224819e+00 4.0229073e+00 3.7616658e+00 3.5683341e+00\n",
      " 3.5409808e+00 3.4230766e+00 3.0704174e+00 2.9637496e+00 2.5906513e+00\n",
      " 2.4415445e+00 2.2460196e+00 2.1945729e+00 2.1862931e+00 2.1392334e+00\n",
      " 2.1094224e+00 2.0721884e+00 2.0611942e+00 1.9725673e+00 1.9698145e+00\n",
      " 1.9002155e+00 1.8520746e+00 1.7742220e+00 1.7380220e+00 1.6798745e+00\n",
      " 1.6563838e+00 1.6504862e+00 1.6122514e+00 1.5930309e+00 1.5709370e+00\n",
      " 1.5078831e+00 1.4569354e+00 1.4416400e+00 1.4152732e+00 1.3497182e+00\n",
      " 1.2640692e+00 1.1941128e+00 1.1857948e+00 1.1185859e+00 1.0879353e+00\n",
      " 1.0583104e+00 1.0467225e+00 9.9971879e-01 9.8576742e-01 9.8371118e-01\n",
      " 9.3481302e-01 9.0860260e-01 8.7651283e-01 8.5576111e-01 8.0939525e-01\n",
      " 8.0120230e-01 7.5030273e-01 7.3237348e-01 7.0446926e-01 6.8925864e-01\n",
      " 6.4973545e-01 6.3680881e-01 6.0080600e-01 5.8611226e-01 5.5128527e-01\n",
      " 5.4156488e-01 4.6133727e-01 4.5960739e-01 4.4949254e-01 4.2347822e-01\n",
      " 3.9797175e-01 3.3266965e-01 3.1455141e-01 2.9044387e-01 2.8229138e-01\n",
      " 2.7632284e-01 2.4874043e-01 2.0643091e-01 2.0503525e-01 1.7614689e-01\n",
      " 1.6286343e-01 1.4780632e-01 1.4444943e-01 1.3237089e-01 9.5554128e-02\n",
      " 7.1757957e-02 3.8730793e-02 7.6407641e-03 6.3441731e-03 5.3490112e-03\n",
      " 0.0000000e+00]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import RocCurveDisplay, roc_curve\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(\n",
    "    np.concatenate((np.ones(len(dist_train)), np.zeros(len(dist_test)))),\n",
    "    dist_train + dist_test\n",
    ")\n",
    "print(fpr)\n",
    "print(tpr)\n",
    "print(thresholds)\n",
    "\n",
    "RocCurveDisplay.from_predictions(\n",
    "    np.concatenate((np.ones(len(dist_train)), np.zeros(len(dist_test)))),\n",
    "    dist_train + dist_test\n",
    ")\n",
    "plt.title(f'{dataset} - {algorithm}')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9fb99d",
   "metadata": {},
   "source": [
    "### 7. Measure the Statistical Relationship between Distance to Decision Boundary and Confidence Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b331301c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train correlation  0.5833840852903146 1.8966935333512727e-10\n",
      "test correlation  0.4844610691914139 3.269388176159646e-07\n"
     ]
    }
   ],
   "source": [
    "# from scipy.stats.stats import pearsonr\n",
    "import scipy\n",
    "\n",
    "confidence_train = [np.amax(scipy.special.softmax(p)) for p in art_classifier.predict(x_target_train[train_idx])]\n",
    "confidence_test = [np.amax(scipy.special.softmax(p)) for p in art_classifier.predict(x_target_test[test_idx])]\n",
    "\n",
    "corr, pvalue = pearsonr(confidence_train, dist_train)\n",
    "print(\"train correlation \", corr, pvalue)\n",
    "corr, pvalue = pearsonr(confidence_test, dist_test)\n",
    "print(\"test correlation \", corr, pvalue)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
